{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67e0b4f0",
   "metadata": {},
   "source": [
    "# **Notas Parcial 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e7a2e9",
   "metadata": {},
   "source": [
    "### **Árboles de decisión**\n",
    "\n",
    "Un árbol de decisión es un modelo que ayuda a realizar predicciones a través de la división de niveles. Es decir, cada nivel del árbol se enfoca en una característica diferente, y esto se clasifica mediante un umbral (dependiendo si es mayor o menor a este)\n",
    "\n",
    "### Split para árboles de regresión\n",
    "\n",
    "En el modelo de regresión con árboles de decisión, el algoritmo divide los datos (dependiendo del umbral) y busca minimiza la varianza en cada nodo. Esto se realiza a través de la siguiente ecuación: \n",
    "\n",
    "$$\n",
    "\\text{Reducción de varianza} = \\text{Varianza total} - \\left( \\frac{N_1}{N} \\times \\text{Var}(G1) + \\frac{N_2}{N} \\times \\text{Var}(G2) \\right)\n",
    "$$\n",
    "\n",
    "Se calcula la reducción de la varianza para cada umbral, y elige el valor que maximice esta reducción. Se repite el proceso en cada nodo hasta cumplir con una condición de parada. \n",
    "\n",
    "### Árboles de clasificación\n",
    "\n",
    "En el modelo de clasificación con árboles de decisión, el algoritmo divide los datos (dependiendo del umbral) y busca minimizar la impureza en cada nodo, esto ya sea con el índice Gini o Entropía. Esto se realiza a través de la siguiente ecuación:\n",
    "\n",
    "$$\n",
    "\\text{Ganancia de Impureza} = \\text{Impureza Nodo Padre} - \\left( \\frac{N_1}{N} \\times \\text{Impureza}(G1) + \\frac{N_2}{N} \\times \\text{Impureza}(G2) \\right)\n",
    "$$\n",
    "\n",
    "Se calcula la reducción de la impureza para cada umbral, y elige el valor que maximice esta reducción. Se repite el proceso en cada nodo hasta cumplir con una condición de parada (como max depth, mínimo de muestras, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13930f23",
   "metadata": {},
   "source": [
    "### **Random Forest Regresión**\n",
    "\n",
    "Random forest es un algoritmo donde se tiene un conjunto de árboles de decisión para buscar predecir algo, a través de la combinación de los resultados de todos los árboles. Se toman muestras aleatorias de los datos (bootstrap), y con cada muestra se entrenan los árboles de decisión. Cada uno de estos árboles realiza una predicción y se promedia esto y se obtiene el resultado.\n",
    "\n",
    "**Esto sirve para obtener una predicción más precisa y evitar el overfitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cdbbeb",
   "metadata": {},
   "source": [
    "### **Gradient Boosting**\n",
    "\n",
    "Es un modelo de predicción donde se netrena un conjunto de árboles de decisión. Se toma la predicción de un árbol de decisión inicial y se calculan los residuos (errores) de este, entre las predicciones y los valores reales. Luego, se entrena un nuevo árbol para predecir los residuos, se corrigen los errores y se actualiza la predicción anterior con la mejorada. Eso se realiza a través de un proceso iterativo, donde hay un factor de aprendizaje que ayuda a disminuir el error, y se obtiene el menor error posible, además, se suman las predicciones de los nuevos árboles. La predicción de cada árbol se calcula de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\gamma_j = \\frac{\\sum \\text{Residuals}}{\\sum_{i \\in R_j} \\text{Previous probability} (1 - \\text{Previous probability})}\n",
    "$$\n",
    "\n",
    " En el caso de clasificación, se utilizan los log odds, que luego se convienten a probabilidades con la siguiente función sigmoide:\n",
    "\n",
    " $$p_i = \\frac{1}{1 + e^{-F_{m}(x_i)}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce52ed71",
   "metadata": {},
   "source": [
    "### **XGBoost**\n",
    "\n",
    "XGBoost es un modelo donde se suman árboles de decisión para lograr mejorar las predicciones. \n",
    "\n",
    "$$\n",
    "\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i)\n",
    "$$\n",
    "\n",
    "Aquí, se tiene lo siguiente:\n",
    "- $\\hat{y}_i^{(t)}$ será la predicción\n",
    "- $f_t(x_i)$ es el nuevo árbol que se entrena\n",
    "- $\\hat{y}_i^{(t-1)}$ es la predicción anterior\n",
    "\n",
    "Esto se realiza de manera iterativa, es decir, se suma un árbol nuevo en cada paso. Esto se hace para minimizar la contribución marginal a la función de pérdida. Se busca reducir el error de los árboles anteriores. La función de pérdida se compone de dos partes (árboles anteriores y árbol nuevo), se debe de optimizar la parte del árbol nuevo, ya que la primera parte se utiliza como constante. Se llega a lo siguiente:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} =\n",
    "\\sum_{i=1}^n l(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) + \\sum_{k=1}^{t} \\Omega(f_k)\n",
    "$$\n",
    "\n",
    "Aquí, se tiene que:\n",
    "\n",
    "$$\n",
    "\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2\n",
    "$$\n",
    "\n",
    "Es decir, $\\Omega(f)$ es un factor de regularización que penaliza árboles más complejos y ayuda a evitar el overfitting. \n",
    "\n",
    "Se utiliza la expansión de Taylor de segundo orden para poder minimizar la pérdida de manera mucho más fácil y se manera cuadrática, porque es una ecuación compleja. La función de pérdida se queda de la siguiente manera (considerando Taylor, el gradiente y el hessiano): \n",
    "\n",
    "$$\n",
    "\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^n \\left[ g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2 \\right] + \\Omega(f_t)\n",
    "$$\n",
    "\n",
    "Con esto, a cada árbol  $f_t$ se le pone un valor constante de  $w_j$ a cada región $R_j$. Esto permite que la función de pérdida, ya simplificada, sea la siguiente: \n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} \\approx \\sum_{j=1}^{T} \\left[\n",
    "G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2\n",
    "\\right] + \\gamma T\n",
    "$$\n",
    "\n",
    "Para obtener el output value, se necesita minimizar por respecto a $w_j$, esto a través de una derivada e igualando a 0.\n",
    "\n",
    "$$\n",
    "\\frac{d}{dw_j} \\left( G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2 \\right) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{Output value} = w_j^* = -\\frac{G_j}{H_j + \\lambda}\n",
    "$$\n",
    "\n",
    "Con este valor de $w_j^*$, se sustituye para obtener el similarity score:\n",
    "\n",
    "Sustituyendo:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_j(w_j^*) =\n",
    "G_j \\left(-\\frac{G_j}{H_j + \\lambda} \\right) +\n",
    "\\frac{1}{2}(H_j + \\lambda) \\left( \\frac{G_j^2}{(H_j + \\lambda)^2} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\frac{G_j^2}{H_j + \\lambda} + \\frac{1}{2} \\cdot \\frac{G_j^2}{H_j + \\lambda}\n",
    "= -\\frac{1}{2} \\cdot \\frac{G_j^2}{H_j + \\lambda}\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\n",
    "\\text{Similarity Score} = \\frac{G_j^2}{H_j + \\lambda}\n",
    "}\n",
    "$$\n",
    "\n",
    "Esto nos sirve para ver que tan buena es una hoja, y si es útil mantenerla o hacerle split.\n",
    "\n",
    "Para saber si vale la pena dividir una hoja en dos, se calcula el gain del split\n",
    "\n",
    "$$\n",
    "\\text{Gain} = \\frac{1}{2} \\left( \\text{Similarity}_\\text{izq} + \\text{Similarity}_\\text{der} - \\text{Similarity}_\\text{padre} \\right) - \\gamma\n",
    "$$\n",
    "\n",
    "Si este valor es positivo, se hace el split, y $\\gamma$ será un hiperparámetro\n",
    "\n",
    "La predicción final se obtiene sumando todos los árboles de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = F_0(x_i) + \\sum_{t=1}^{M} f_t(x_i)\n",
    "$$\n",
    "\n",
    "Cabe mencionar que si es clasificación, se aplica la función sigmoide para convertirlo a probabilidad:\n",
    "\n",
    "$$\n",
    "p_i = \\frac{1}{1 + e^{-\\hat{y}_i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585c6e0c",
   "metadata": {},
   "source": [
    "### **Feature Importance**\n",
    "\n",
    "**¿Qué es feature importance?**\n",
    "\n",
    "Feature importance es una técnica con la cual se evalua la importancia o que tan relevante es una variable para que el modelo haga sus predicciones\n",
    "\n",
    "\n",
    "WEIGHT / SPLIT: cuantas veces se utiliza cada variable\n",
    "GAIN: en promedio cuanto baja la función de pérdida cuando utilizas una variable (normalizado de 0 a 1)\n",
    "\n",
    "LightGBM:\n",
    "\n",
    "lgb.plot_importance(model, max_num_features = 10, importance_type='gain')\n",
    "lgb.plot_importance(model, max_num_features = 10, importance_type='split')\n",
    "\n",
    "XGBoost:\n",
    "\n",
    "plot_importance(model, max_num_features=10, importance_type=\"weight\")\n",
    "plot_importance(model, max_num_features=10, importance_type=\"gain\")\n",
    "\n",
    "CatBoost:\n",
    "\n",
    "df_importances = pd.DataFrame({\n",
    "    'feature': model.feature_names_,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "df_importances\n",
    "\n",
    "FALTARÍ RANDOM FOREST\n",
    "\n",
    "\n",
    "**Partial dependency plot**\n",
    "\n",
    "El partial dependency plot muestra como cambia la predicción del modelo cuando una variable toma distintos valores y las otras variables se mantienen fijas\n",
    "\n",
    "\n",
    "*EJEMPLO:*\n",
    "\n",
    "imaginá que tenés un modelo de predicción de precio de casas, y hacés un PDP para la variable num_habitaciones. El gráfico te muestra:\n",
    "\n",
    "\"Si todas las casas tuvieran 2 habitaciones, ¿cuál sería el precio promedio según el modelo? ¿Y si todas tuvieran 3? ¿4?\"\n",
    "\n",
    "Esto te permite ver si, por ejemplo, el modelo aprende que más habitaciones = mayor precio, o si hay un punto donde el efecto se aplana o incluso baja."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc8694e",
   "metadata": {},
   "source": [
    "### **Comparativa**\n",
    "\n",
    "La estructura del árbol:\n",
    "\n",
    "* XGBoost produce árboles más simétricos y balanceados.\n",
    "\n",
    "* LightGBM produce árboles más profundos y desbalanceados si no se controla.\n",
    "\n",
    "La precisión y riesgo de overfitting:\n",
    "\n",
    "* Leaf-wise (LightGBM) puede encontrar mejores divisiones, pero se sobreajusta más fácil\n",
    "\n",
    "* Level-wise (XGBoost) es más estable, pero a veces menos preciso.\n",
    "\n",
    "Diferencias generales:\n",
    "* light gbm no crece de manera ordenada, ya que hace un histograma de 250 bins y no uno por uno\n",
    "\n",
    "* en catboost las ramas son iguales, lo cual hace que la prediccion sea rápida\n",
    "\n",
    "* catboost es mejor cuando hay variables categóricas, ya que realiza categorical encoding \n",
    "\n",
    "* en catboost las ramas son iguales, hace que la prediccion sea rapida\n",
    "\n",
    "* catboost: lo especial de esto es el encoding (categorical boosting) esta caracterizada en features categoricas\n",
    "\n",
    "* catboost: promedio de manera secuencial\n",
    "\n",
    "* lo que hace catboost es que el encoding de cada variable en cada punto tenga que ver con el promedio conocido hasta cierto momento\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Característica              | **XGBoost**                                                | **LightGBM**                                               | **CatBoost**                                                  |\n",
    "|-----------------------------|------------------------------------------------------------|-------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Velocidad**               | Rápido, pero más lento que LightGBM y CatBoost             | 🔥 Muy rápido gracias a histogramas y leaf-wise growth      | Rápido, aunque un poco más lento que LightGBM                  |\n",
    "| **Precisión**               | Alta                                                       | Alta, a veces mejor con buen tuning                         | Muy alta, especialmente con categóricas                        |\n",
    "| **Variables categóricas**   | ❌ No las maneja (requiere encoding manual)                | ❌ No las maneja (requiere encoding manual)                 | ✅ Soporte nativo + regularización secuencial                  |\n",
    "| **Uso de memoria**          | Moderado                                                   | ✅ Muy eficiente (binning)                                   | Similar a XGBoost                                              |\n",
    "| **Manejo de missing values**| ✅ Automático                                               | ✅ Automático                                                | ✅ Automático                                                   |\n",
    "| **Soporte GPU**             | ✅ Sí (bastante estable)                                   | ✅ Sí (muy rápido)                                           | ✅ Sí (algo más limitado)                                      |\n",
    "| **Instalación**             | Fácil (`pip install xgboost`)                             | Fácil (`pip install lightgbm`)                              | Un poco más pesada (`pip install catboost`)                   |\n",
    "| **Documentación**           | Excelente                                                  | Buena                                                       | Muy buena                                                     |\n",
    "| **Interacción con sklearn** | Muy buena                                                  | Muy buena                                                   | Muy buena                                                     |\n",
    "| **Tolerancia al orden**     | ✅ Neutral                                                  | ✅ Neutral                                                   | ⚠️ Sensible (por codificación secuencial)                      |\n",
    "\n",
    "\n",
    "**XGBoost**:\n",
    "\n",
    "Optimiza una función de pérdida regularizada (como logística + penalización).\n",
    "\n",
    "Usa expansión de Taylor de segundo orden → usa derivadas de segundo orden (Hessiana).\n",
    "\n",
    "\n",
    "**LightGBM**:\n",
    "\n",
    "Histograma binning: agrupa valores continuos en bins → acelera entrenamiento.\n",
    "\n",
    "Crece los árboles leaf-wise con \"depth limit\".\n",
    "\n",
    "\n",
    "**CatBoost**:\n",
    "\n",
    "Usa Ordered Boosting: al entrenar, simula que los datos vienen en orden temporal → evita \"target leakage\".\n",
    "\n",
    "Maneja categóricas con combinaciones de variables y estadísticos del conjunto \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1608276c",
   "metadata": {},
   "source": [
    "### **Prueba de poder**\n",
    "\n",
    "Una prueba de poder nos permite saber cuantos datos se necesitan en un AB test para confiar en los resultados, es decir, para que realmente se vea un cambio o efecto en la muestra. \n",
    "\n",
    "La prueba Z nos ayuda saber si una media o proporción observada difere significativamente de un valor esperado\n",
    "\n",
    "$H_0$: No hay diferencia significativa\n",
    "$H_A$: Si hay diferencia significativa\n",
    "\n",
    "Cuando p-value es menor a 0.05, se rechaza la hipótesis nula, es decir la media de la muestra es significativamente diferente a la media\n",
    "\n",
    "Existen dos tipos de errores en la prueba de poder:\n",
    "* Error tipo 1 $\\alpha$: Falso positivo (rechazas $H_0$ cuando en realidad es verdadera)\n",
    "* Error tipo 2 $\\beta$: Falso negativo (no se rechaza $H_0$ cuando en realidad es $H_A$)\n",
    "\n",
    "**Power Analysis (Análisis de Potencia)**\n",
    "El power analysis es una técnica estadística utilizada para determinar el tamaño de muestra (N) necesario para detectar un efecto de interés con una probabilidad específica. También se puede usar para calcular la potencia de una prueba dada una muestra y un tamaño de efecto.\n",
    "\n",
    "En términos simples, nos ayuda a responder preguntas como:\n",
    "\n",
    "- ¿Cuántas observaciones necesito para detectar un efecto real con alta probabilidad?\n",
    "- ¿Qué tan probable es que mi estudio detecte un efecto real si realmente existe?\n",
    "\n",
    "### Elementos clave\n",
    "- Potencia : (1 - $\\beta$) Es la probabilidad de rechazar $H_0$ cuando $H_A$ es negativa (Error tipo II). Usualmente queremos tener $\\beta$ en 80%\n",
    "- Nivel de significancia: $\\alpha$ Es la probabilidad de rechazar $H_0$ cuando $H_0$ es verdadera (error tipo I)\n",
    "\n",
    "**LO MATEMÁTICO Y EL CÓDIGO ESTÁ EN EL ARCHIVO PRUEBA_DE_PODER**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7f7377",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
