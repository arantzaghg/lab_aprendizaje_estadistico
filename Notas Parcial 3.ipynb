{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67e0b4f0",
   "metadata": {},
   "source": [
    "# **Notas Parcial 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e7a2e9",
   "metadata": {},
   "source": [
    "### **√Årboles de decisi√≥n**\n",
    "\n",
    "Un √°rbol de decisi√≥n es un modelo que ayuda a realizar predicciones a trav√©s de la divisi√≥n de niveles. Es decir, cada nivel del √°rbol se enfoca en una caracter√≠stica diferente, y esto se clasifica mediante un umbral (dependiendo si es mayor o menor a este)\n",
    "\n",
    "### Split para √°rboles de regresi√≥n\n",
    "\n",
    "En el modelo de regresi√≥n con √°rboles de decisi√≥n, el algoritmo divide los datos (dependiendo del umbral) y busca minimiza la varianza en cada nodo. Esto se realiza a trav√©s de la siguiente ecuaci√≥n: \n",
    "\n",
    "$$\n",
    "\\text{Reducci√≥n de varianza} = \\text{Varianza total} - \\left( \\frac{N_1}{N} \\times \\text{Var}(G1) + \\frac{N_2}{N} \\times \\text{Var}(G2) \\right)\n",
    "$$\n",
    "\n",
    "Se calcula la reducci√≥n de la varianza para cada umbral, y elige el valor que maximice esta reducci√≥n. Se repite el proceso en cada nodo hasta cumplir con una condici√≥n de parada. \n",
    "\n",
    "### √Årboles de clasificaci√≥n\n",
    "\n",
    "En el modelo de clasificaci√≥n con √°rboles de decisi√≥n, el algoritmo divide los datos (dependiendo del umbral) y busca minimizar la impureza en cada nodo, esto ya sea con el √≠ndice Gini o Entrop√≠a. Esto se realiza a trav√©s de la siguiente ecuaci√≥n:\n",
    "\n",
    "$$\n",
    "\\text{Ganancia de Impureza} = \\text{Impureza Nodo Padre} - \\left( \\frac{N_1}{N} \\times \\text{Impureza}(G1) + \\frac{N_2}{N} \\times \\text{Impureza}(G2) \\right)\n",
    "$$\n",
    "\n",
    "Se calcula la reducci√≥n de la impureza para cada umbral, y elige el valor que maximice esta reducci√≥n. Se repite el proceso en cada nodo hasta cumplir con una condici√≥n de parada (como max depth, m√≠nimo de muestras, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13930f23",
   "metadata": {},
   "source": [
    "### **Random Forest Regresi√≥n**\n",
    "\n",
    "Random forest es un algoritmo donde se tiene un conjunto de √°rboles de decisi√≥n para buscar predecir algo, a trav√©s de la combinaci√≥n de los resultados de todos los √°rboles. Se toman muestras aleatorias de los datos (bootstrap), y con cada muestra se entrenan los √°rboles de decisi√≥n. Cada uno de estos √°rboles realiza una predicci√≥n y se promedia esto y se obtiene el resultado.\n",
    "\n",
    "**Esto sirve para obtener una predicci√≥n m√°s precisa y evitar el overfitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cdbbeb",
   "metadata": {},
   "source": [
    "### **Gradient Boosting**\n",
    "\n",
    "Es un modelo de predicci√≥n donde se netrena un conjunto de √°rboles de decisi√≥n. Se toma la predicci√≥n de un √°rbol de decisi√≥n inicial y se calculan los residuos (errores) de este, entre las predicciones y los valores reales. Luego, se entrena un nuevo √°rbol para predecir los residuos, se corrigen los errores y se actualiza la predicci√≥n anterior con la mejorada. Eso se realiza a trav√©s de un proceso iterativo, donde hay un factor de aprendizaje que ayuda a disminuir el error, y se obtiene el menor error posible, adem√°s, se suman las predicciones de los nuevos √°rboles. La predicci√≥n de cada √°rbol se calcula de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\gamma_j = \\frac{\\sum \\text{Residuals}}{\\sum_{i \\in R_j} \\text{Previous probability} (1 - \\text{Previous probability})}\n",
    "$$\n",
    "\n",
    " En el caso de clasificaci√≥n, se utilizan los log odds, que luego se convienten a probabilidades con la siguiente funci√≥n sigmoide:\n",
    "\n",
    " $$p_i = \\frac{1}{1 + e^{-F_{m}(x_i)}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce52ed71",
   "metadata": {},
   "source": [
    "### **XGBoost**\n",
    "\n",
    "XGBoost es un modelo donde se suman √°rboles de decisi√≥n para lograr mejorar las predicciones. \n",
    "\n",
    "$$\n",
    "\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i)\n",
    "$$\n",
    "\n",
    "Aqu√≠, se tiene lo siguiente:\n",
    "- $\\hat{y}_i^{(t)}$ ser√° la predicci√≥n\n",
    "- $f_t(x_i)$ es el nuevo √°rbol que se entrena\n",
    "- $\\hat{y}_i^{(t-1)}$ es la predicci√≥n anterior\n",
    "\n",
    "Esto se realiza de manera iterativa, es decir, se suma un √°rbol nuevo en cada paso. Esto se hace para minimizar la contribuci√≥n marginal a la funci√≥n de p√©rdida. Se busca reducir el error de los √°rboles anteriores. La funci√≥n de p√©rdida se compone de dos partes (√°rboles anteriores y √°rbol nuevo), se debe de optimizar la parte del √°rbol nuevo, ya que la primera parte se utiliza como constante. Se llega a lo siguiente:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} =\n",
    "\\sum_{i=1}^n l(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) + \\sum_{k=1}^{t} \\Omega(f_k)\n",
    "$$\n",
    "\n",
    "Aqu√≠, se tiene que:\n",
    "\n",
    "$$\n",
    "\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2\n",
    "$$\n",
    "\n",
    "Es decir, $\\Omega(f)$ es un factor de regularizaci√≥n que penaliza √°rboles m√°s complejos y ayuda a evitar el overfitting. \n",
    "\n",
    "Se utiliza la expansi√≥n de Taylor de segundo orden para poder minimizar la p√©rdida de manera mucho m√°s f√°cil y se manera cuadr√°tica, porque es una ecuaci√≥n compleja. La funci√≥n de p√©rdida se queda de la siguiente manera (considerando Taylor, el gradiente y el hessiano): \n",
    "\n",
    "$$\n",
    "\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^n \\left[ g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2 \\right] + \\Omega(f_t)\n",
    "$$\n",
    "\n",
    "Con esto, a cada √°rbol  $f_t$ se le pone un valor constante de  $w_j$ a cada regi√≥n $R_j$. Esto permite que la funci√≥n de p√©rdida, ya simplificada, sea la siguiente: \n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} \\approx \\sum_{j=1}^{T} \\left[\n",
    "G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2\n",
    "\\right] + \\gamma T\n",
    "$$\n",
    "\n",
    "Para obtener el output value, se necesita minimizar por respecto a $w_j$, esto a trav√©s de una derivada e igualando a 0.\n",
    "\n",
    "$$\n",
    "\\frac{d}{dw_j} \\left( G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2 \\right) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{Output value} = w_j^* = -\\frac{G_j}{H_j + \\lambda}\n",
    "$$\n",
    "\n",
    "Con este valor de $w_j^*$, se sustituye para obtener el similarity score:\n",
    "\n",
    "Sustituyendo:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_j(w_j^*) =\n",
    "G_j \\left(-\\frac{G_j}{H_j + \\lambda} \\right) +\n",
    "\\frac{1}{2}(H_j + \\lambda) \\left( \\frac{G_j^2}{(H_j + \\lambda)^2} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\frac{G_j^2}{H_j + \\lambda} + \\frac{1}{2} \\cdot \\frac{G_j^2}{H_j + \\lambda}\n",
    "= -\\frac{1}{2} \\cdot \\frac{G_j^2}{H_j + \\lambda}\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\n",
    "\\text{Similarity Score} = \\frac{G_j^2}{H_j + \\lambda}\n",
    "}\n",
    "$$\n",
    "\n",
    "Esto nos sirve para ver que tan buena es una hoja, y si es √∫til mantenerla o hacerle split.\n",
    "\n",
    "Para saber si vale la pena dividir una hoja en dos, se calcula el gain del split\n",
    "\n",
    "$$\n",
    "\\text{Gain} = \\frac{1}{2} \\left( \\text{Similarity}_\\text{izq} + \\text{Similarity}_\\text{der} - \\text{Similarity}_\\text{padre} \\right) - \\gamma\n",
    "$$\n",
    "\n",
    "Si este valor es positivo, se hace el split, y $\\gamma$ ser√° un hiperpar√°metro\n",
    "\n",
    "La predicci√≥n final se obtiene sumando todos los √°rboles de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = F_0(x_i) + \\sum_{t=1}^{M} f_t(x_i)\n",
    "$$\n",
    "\n",
    "Cabe mencionar que si es clasificaci√≥n, se aplica la funci√≥n sigmoide para convertirlo a probabilidad:\n",
    "\n",
    "$$\n",
    "p_i = \\frac{1}{1 + e^{-\\hat{y}_i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585c6e0c",
   "metadata": {},
   "source": [
    "### **Feature Importance**\n",
    "\n",
    "**¬øQu√© es feature importance?**\n",
    "\n",
    "Feature importance es una t√©cnica con la cual se evalua la importancia o que tan relevante es una variable para que el modelo haga sus predicciones\n",
    "\n",
    "\n",
    "WEIGHT / SPLIT: cuantas veces se utiliza cada variable\n",
    "GAIN: en promedio cuanto baja la funci√≥n de p√©rdida cuando utilizas una variable (normalizado de 0 a 1)\n",
    "\n",
    "LightGBM:\n",
    "\n",
    "lgb.plot_importance(model, max_num_features = 10, importance_type='gain')\n",
    "lgb.plot_importance(model, max_num_features = 10, importance_type='split')\n",
    "\n",
    "XGBoost:\n",
    "\n",
    "plot_importance(model, max_num_features=10, importance_type=\"weight\")\n",
    "plot_importance(model, max_num_features=10, importance_type=\"gain\")\n",
    "\n",
    "CatBoost:\n",
    "\n",
    "df_importances = pd.DataFrame({\n",
    "    'feature': model.feature_names_,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "df_importances\n",
    "\n",
    "FALTAR√ç RANDOM FOREST\n",
    "\n",
    "\n",
    "**Partial dependency plot**\n",
    "\n",
    "El partial dependency plot muestra como cambia la predicci√≥n del modelo cuando una variable toma distintos valores y las otras variables se mantienen fijas\n",
    "\n",
    "\n",
    "*EJEMPLO:*\n",
    "\n",
    "imagin√° que ten√©s un modelo de predicci√≥n de precio de casas, y hac√©s un PDP para la variable num_habitaciones. El gr√°fico te muestra:\n",
    "\n",
    "\"Si todas las casas tuvieran 2 habitaciones, ¬øcu√°l ser√≠a el precio promedio seg√∫n el modelo? ¬øY si todas tuvieran 3? ¬ø4?\"\n",
    "\n",
    "Esto te permite ver si, por ejemplo, el modelo aprende que m√°s habitaciones = mayor precio, o si hay un punto donde el efecto se aplana o incluso baja."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc8694e",
   "metadata": {},
   "source": [
    "### **Comparativa**\n",
    "\n",
    "La estructura del √°rbol:\n",
    "\n",
    "* XGBoost produce √°rboles m√°s sim√©tricos y balanceados.\n",
    "\n",
    "* LightGBM produce √°rboles m√°s profundos y desbalanceados si no se controla.\n",
    "\n",
    "La precisi√≥n y riesgo de overfitting:\n",
    "\n",
    "* Leaf-wise (LightGBM) puede encontrar mejores divisiones, pero se sobreajusta m√°s f√°cil\n",
    "\n",
    "* Level-wise (XGBoost) es m√°s estable, pero a veces menos preciso.\n",
    "\n",
    "Diferencias generales:\n",
    "* light gbm no crece de manera ordenada, ya que hace un histograma de 250 bins y no uno por uno\n",
    "\n",
    "* en catboost las ramas son iguales, lo cual hace que la prediccion sea r√°pida\n",
    "\n",
    "* catboost es mejor cuando hay variables categ√≥ricas, ya que realiza categorical encoding \n",
    "\n",
    "* en catboost las ramas son iguales, hace que la prediccion sea rapida\n",
    "\n",
    "* catboost: lo especial de esto es el encoding (categorical boosting) esta caracterizada en features categoricas\n",
    "\n",
    "* catboost: promedio de manera secuencial\n",
    "\n",
    "* lo que hace catboost es que el encoding de cada variable en cada punto tenga que ver con el promedio conocido hasta cierto momento\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Caracter√≠stica              | **XGBoost**                                                | **LightGBM**                                               | **CatBoost**                                                  |\n",
    "|-----------------------------|------------------------------------------------------------|-------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Velocidad**               | R√°pido, pero m√°s lento que LightGBM y CatBoost             | üî• Muy r√°pido gracias a histogramas y leaf-wise growth      | R√°pido, aunque un poco m√°s lento que LightGBM                  |\n",
    "| **Precisi√≥n**               | Alta                                                       | Alta, a veces mejor con buen tuning                         | Muy alta, especialmente con categ√≥ricas                        |\n",
    "| **Variables categ√≥ricas**   | ‚ùå No las maneja (requiere encoding manual)                | ‚ùå No las maneja (requiere encoding manual)                 | ‚úÖ Soporte nativo + regularizaci√≥n secuencial                  |\n",
    "| **Uso de memoria**          | Moderado                                                   | ‚úÖ Muy eficiente (binning)                                   | Similar a XGBoost                                              |\n",
    "| **Manejo de missing values**| ‚úÖ Autom√°tico                                               | ‚úÖ Autom√°tico                                                | ‚úÖ Autom√°tico                                                   |\n",
    "| **Soporte GPU**             | ‚úÖ S√≠ (bastante estable)                                   | ‚úÖ S√≠ (muy r√°pido)                                           | ‚úÖ S√≠ (algo m√°s limitado)                                      |\n",
    "| **Instalaci√≥n**             | F√°cil (`pip install xgboost`)                             | F√°cil (`pip install lightgbm`)                              | Un poco m√°s pesada (`pip install catboost`)                   |\n",
    "| **Documentaci√≥n**           | Excelente                                                  | Buena                                                       | Muy buena                                                     |\n",
    "| **Interacci√≥n con sklearn** | Muy buena                                                  | Muy buena                                                   | Muy buena                                                     |\n",
    "| **Tolerancia al orden**     | ‚úÖ Neutral                                                  | ‚úÖ Neutral                                                   | ‚ö†Ô∏è Sensible (por codificaci√≥n secuencial)                      |\n",
    "\n",
    "\n",
    "**XGBoost**:\n",
    "\n",
    "Optimiza una funci√≥n de p√©rdida regularizada (como log√≠stica + penalizaci√≥n).\n",
    "\n",
    "Usa expansi√≥n de Taylor de segundo orden ‚Üí usa derivadas de segundo orden (Hessiana).\n",
    "\n",
    "\n",
    "**LightGBM**:\n",
    "\n",
    "Histograma binning: agrupa valores continuos en bins ‚Üí acelera entrenamiento.\n",
    "\n",
    "Crece los √°rboles leaf-wise con \"depth limit\".\n",
    "\n",
    "\n",
    "**CatBoost**:\n",
    "\n",
    "Usa Ordered Boosting: al entrenar, simula que los datos vienen en orden temporal ‚Üí evita \"target leakage\".\n",
    "\n",
    "Maneja categ√≥ricas con combinaciones de variables y estad√≠sticos del conjunto \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1608276c",
   "metadata": {},
   "source": [
    "### **Prueba de poder**\n",
    "\n",
    "Una prueba de poder nos permite saber cuantos datos se necesitan en un AB test para confiar en los resultados, es decir, para que realmente se vea un cambio o efecto en la muestra. \n",
    "\n",
    "La prueba Z nos ayuda saber si una media o proporci√≥n observada difere significativamente de un valor esperado\n",
    "\n",
    "$H_0$: No hay diferencia significativa\n",
    "$H_A$: Si hay diferencia significativa\n",
    "\n",
    "Cuando p-value es menor a 0.05, se rechaza la hip√≥tesis nula, es decir la media de la muestra es significativamente diferente a la media\n",
    "\n",
    "Existen dos tipos de errores en la prueba de poder:\n",
    "* Error tipo 1 $\\alpha$: Falso positivo (rechazas $H_0$ cuando en realidad es verdadera)\n",
    "* Error tipo 2 $\\beta$: Falso negativo (no se rechaza $H_0$ cuando en realidad es $H_A$)\n",
    "\n",
    "**Power Analysis (An√°lisis de Potencia)**\n",
    "El power analysis es una t√©cnica estad√≠stica utilizada para determinar el tama√±o de muestra (N) necesario para detectar un efecto de inter√©s con una probabilidad espec√≠fica. Tambi√©n se puede usar para calcular la potencia de una prueba dada una muestra y un tama√±o de efecto.\n",
    "\n",
    "En t√©rminos simples, nos ayuda a responder preguntas como:\n",
    "\n",
    "- ¬øCu√°ntas observaciones necesito para detectar un efecto real con alta probabilidad?\n",
    "- ¬øQu√© tan probable es que mi estudio detecte un efecto real si realmente existe?\n",
    "\n",
    "###¬†Elementos clave\n",
    "- Potencia : (1 - $\\beta$) Es la probabilidad de rechazar $H_0$ cuando $H_A$ es negativa (Error tipo II). Usualmente queremos tener $\\beta$ en 80%\n",
    "- Nivel de significancia: $\\alpha$ Es la probabilidad de rechazar $H_0$ cuando $H_0$ es verdadera (error tipo I)\n",
    "\n",
    "**LO MATEM√ÅTICO Y EL C√ìDIGO EST√Å EN EL ARCHIVO PRUEBA_DE_PODER**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7f7377",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
